{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Healthy State Reconstruction Model (GRU) - Modified\n",
    "\n",
    "This notebook implements a GRU-based model to reconstruct/predict the healthy state of 'W' and 'X' sensor data (18 columns). \n",
    "Normalization Stats are computed purely from the Training set to strictly prevent data leakage and saved for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Preprocessing (Strict No-Leakage)\n",
    "\n",
    "- Load Parquet file\n",
    "- Determine Window Indices\n",
    "- **Split Train/Val by UNIT (Reserve 1 unit for Val, rest for Train)**\n",
    "- **Fit Scaler ONLY on Training Units**\n",
    "- Transform Data\n",
    "- Create Dataset (Returns full 18-col window and 18-col target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data_path = r'c:\\Users\\Bilge\\OneDrive\\Masa\u00fcst\u00fc\\N-CMAPSS RUL\\healthy state\\DS02_healthyStateTrain.parquet'\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "# Ensure sorted by unit and cycle\n",
    "df = df.sort_values(by=['unit', 'cycle']).reset_index(drop=True)\n",
    "\n",
    "# Define columns\n",
    "w_cols = ['alt', 'Mach', 'TRA', 'T2']\n",
    "x_cols = [c for c in df.columns if c not in ['unit', 'cycle', 'hs'] + w_cols]\n",
    "feature_cols = w_cols + x_cols\n",
    "\n",
    "print(f\"Feature columns ({len(feature_cols)}): {feature_cols}\")\n",
    "\n",
    "# Helper to find valid indices for specific units\n",
    "def get_indices_for_units(data_df, selected_units, window_length=64):\n",
    "    valid_indices = []\n",
    "    grouped = data_df.groupby('unit')\n",
    "    cumulative_len = 0\n",
    "    for unit_id, group in grouped:\n",
    "        group_len = len(group)\n",
    "        if unit_id in selected_units:\n",
    "             if group_len >= window_length:\n",
    "                start_indices = np.arange(cumulative_len, cumulative_len + group_len - window_length + 1)\n",
    "                valid_indices.extend(start_indices)\n",
    "        cumulative_len += group_len\n",
    "    return np.array(valid_indices)\n",
    "\n",
    "# 1. Split Units into Train and Validation\n",
    "units = df['unit'].unique()\n",
    "val_unit = units[-1] # Reserve the last unit for validation\n",
    "train_units = units[:-1]\n",
    "\n",
    "print(f\"All Units: {units}\")\n",
    "print(f\"Train Units: {train_units}\")\n",
    "print(f\"Validation Unit: {val_unit}\")\n",
    "\n",
    "# 2. Normalize Data (Fit ONLY on Training Units)\n",
    "scaler = StandardScaler()\n",
    "train_df_for_scaler = df[df['unit'].isin(train_units)]\n",
    "scaler.fit(train_df_for_scaler[feature_cols])\n",
    "\n",
    "# Save Scaler\n",
    "joblib.dump(scaler, 'normalization_params.pkl')\n",
    "print(\"Scaler saved to 'normalization_params.pkl'\")\n",
    "\n",
    "# Transform the entire dataframe (Validation data is transformed using Train stats)\n",
    "df[feature_cols] = scaler.transform(df[feature_cols])\n",
    "print(\"Normalization completed: Scaler fit on training units only.\")\n",
    "\n",
    "# 3. Get valid window start indices for Train and Val\n",
    "train_indices = get_indices_for_units(df, train_units, window_length=64)\n",
    "val_indices = get_indices_for_units(df, [val_unit], window_length=64)\n",
    "\n",
    "print(f\"Training windows: {len(train_indices)}\")\n",
    "print(f\"Validation windows: {len(val_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create Dataset Class\n",
    "class LazySlidingWindowDataset(Dataset):\n",
    "    def __init__(self, data_df, valid_indices_subset, feature_columns=None, window_length=64, mode='train'):\n",
    "        self.window_length = window_length\n",
    "        self.mode = mode\n",
    "        self.feature_columns = feature_columns\n",
    "        \n",
    "        # Validate feature_columns\n",
    "        if feature_columns is None:\n",
    "            # Fallback to global if not passed (not recommended but for backward compat)\n",
    "            # But better to raise error or warn. Here we assume caller passes it.\n",
    "            raise ValueError(\"feature_columns must be provided\")\n",
    "            \n",
    "        # Store data as a single numpy array to save memory\n",
    "        self.data = data_df[feature_columns].values.astype(np.float32)\n",
    "        # self.targets removed to save memory\n",
    "        \n",
    "        # Use the specific subset of indices (Train or Val) provided\n",
    "        self.valid_indices = valid_indices_subset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.valid_indices[idx]\n",
    "        end_idx = start_idx + self.window_length\n",
    "        \n",
    "        # Input: (64, 18)\n",
    "        window = self.data[start_idx:end_idx].copy()\n",
    "        \n",
    "        # Target: (18,) - the last time step of the window\n",
    "        # The model is trained to reconstruct the last step given the sequence\n",
    "        target = window[-1].copy()\n",
    "        \n",
    "        return torch.from_numpy(window), torch.from_numpy(target)\n",
    "\n",
    "# 5. Instantiate Datasets with the unit-based split indices\n",
    "print(\"Instantiating Datasets...\")\n",
    "train_dataset = LazySlidingWindowDataset(\n",
    "    df, \n",
    "    valid_indices_subset=train_indices, \n",
    "    feature_columns=feature_cols,\n",
    "    window_length=64, \n",
    "    mode='train'\n",
    ")\n",
    "\n",
    "val_dataset = LazySlidingWindowDataset(\n",
    "    df, \n",
    "    valid_indices_subset=val_indices, \n",
    "    feature_columns=feature_cols,\n",
    "    window_length=64, \n",
    "    mode='val'\n",
    ")\n",
    "\n",
    "print(f\"Train Dataset Length: {len(train_dataset)}\")\n",
    "print(f\"Val Dataset Length: {len(val_dataset)}\")\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "print(\"DataLoaders created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "- Input: (64, 18)\n",
    "- GRU 1: 48 units (return_sequences=True)\n",
    "- GRU 2: 16 units (return_sequences=True)\n",
    "- GRU 3: 8 units (return_sequences=False)\n",
    "- Dense 1: 16 units\n",
    "- Dense 2: 24 units\n",
    "- Output: 18 units (Fully Connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUReconstructor(nn.Module):\n",
    "    def __init__(self, input_dim=18, output_dim=18, dropout=0.2):\n",
    "        super(GRUReconstructor, self).__init__()\n",
    "        \n",
    "        self.gru1 = nn.GRU(input_dim, 48, batch_first=True)\n",
    "        self.gru2 = nn.GRU(48, 16, batch_first=True)\n",
    "        self.gru3 = nn.GRU(16, 8, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8, 16)\n",
    "        self.fc2 = nn.Linear(16, 24)\n",
    "        self.fc3 = nn.Linear(24, output_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 64, 18)\n",
    "        \n",
    "        # GRU Layers\n",
    "        out, _ = self.gru1(x) # (batch, 64, 48)\n",
    "        out, _ = self.gru2(out) # (batch, 64, 16)\n",
    "        out, _ = self.gru3(out) # (batch, 64, 8)\n",
    "        \n",
    "        # Take only the last time step\n",
    "        last_step = out[:, -1, :] # (batch, 8)\n",
    "        \n",
    "        # Dense Layers\n",
    "        out = self.relu(self.fc1(last_step))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        output = self.fc3(out) # Linear activation for output\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "\n",
    "- Optimizer: AdamW(weight_decay=0.1)\n",
    "- Loss: MSELoss\n",
    "- Custom Scheduler: Patience=4, reduce LR, load best model. Stop at 3rd trigger.\n",
    "- Early Stopping: Save best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = GRUReconstructor(input_dim=18, output_dim=18).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "patience = 4\n",
    "trigger_count = 0\n",
    "max_triggers = 3\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = 'best_model.pth'\n",
    "epochs_no_improve = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Starting training loop...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_train_loss = running_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "    epoch_val_loss = val_running_loss / len(val_dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_train_loss:.6f} - Val Loss: {epoch_val_loss:.6f}\")\n",
    "    \n",
    "    # Scheduler & Early Stopping Logic\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        # print(\"Saved best model.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement for {epochs_no_improve} epochs.\")\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            trigger_count += 1\n",
    "            print(f\"Trigger {trigger_count}/{max_triggers}: Reducing LR and reloading best model.\")\n",
    "            \n",
    "            if trigger_count >= max_triggers:\n",
    "                print(\"Max triggers reached. Stopping training.\")\n",
    "                break\n",
    "            \n",
    "            # Load best model\n",
    "            model.load_state_dict(torch.load(best_model_path))\n",
    "            \n",
    "            # Reduce LR\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.5 # Reduce by factor of 0.5 (or 0.1)\n",
    "                print(f\"New Learning Rate: {param_group['lr']}\")\n",
    "            \n",
    "            # Reset counter\n",
    "            epochs_no_improve = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}